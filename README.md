# Scalable 3D Captioning with Pretrained Models

[Chris Rockwell*](https://crockwell.github.io), [Tiange Luo*](https://tiangeluo.github.io/), [Honglak Lee†](https://web.eecs.umich.edu/~honglak/), [Justin Johnson†](https://web.eecs.umich.edu/~justincj)

*Equal contribution    †Equal Advising

Data download available at [Hugging Face](https://huggingface.co/datasets/tiange/Cap3D)

<img src="teaser.png" alt="drawing">

## Overview
Cap3D provides detailed descriptions of 3D objects by leveraging pretrained models in
captioning, alignment, and LLM to consolidate multi-view information.

## Code
Full code is coming soon.

## To Do
  - [ ] Captioning Pipeline
  - [ ] Captioning Experiments
  - [ ] Text-to-3D Experiments

## Citation
If you find this code useful, please consider citing:
```
    @article{luo2023scalable,
      author = {Tiange Luo and Chris Rockwell and Honglak Lee and Justin Johnson},
      author={Luo, Tiange and Rockwell, Chris and Lee, Honglak and Johnson, Justin},
      journal={arXiv preprint arXiv:2306.07279},
      year = 2023
    }
```

## Acknowledgments
This work is supported by two grants from LG AI Research and Grant #1453651 from NSF.
Thanks to <a href="https://mbanani.github.io/">Mohamed El Banani</a>, <a href="http://kdexd.xyz/">Karan Desai</a> and <a href="https://nileshkulkarni.github.io/">Ang Cao</a> for their many helpful suggestions. Thanks <a href="https://mattdeitke.com/">Matt Deitke</a> for helping with Objaverse-related questions. 
